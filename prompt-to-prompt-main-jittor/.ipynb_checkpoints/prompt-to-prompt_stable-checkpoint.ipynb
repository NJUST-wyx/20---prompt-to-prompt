{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52eaed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## Copyright 2022 Google LLC. Double-click for license information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "# ## Prompt-to-Prompt with Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from typing import Optional, Union, Tuple, List, Callable, Dict\n",
    "import torch\n",
    "from JDiffusion import StableDiffusionPipeline\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "import abc\n",
    "import ptp_utils\n",
    "import seq_aligner\n",
    "\n",
    "import random\n",
    "import jittor\n",
    "\n",
    "\n",
    "# For loading the Stable Diffusion using Diffusers, follow the instuctions https://huggingface.co/blog/stable_diffusion and update ```MY_TOKEN``` with your token.\n",
    "# Set ```LOW_RESOURCE``` to ```True``` for running on 12GB GPU.\n",
    "\n",
    "def set_all_random_seeds(seed=42):\n",
    "    # 固定Python内置随机模块\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # 固定NumPy随机种子\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # # 固定PyTorch随机种子\n",
    "    # torch.manual_seed(seed)\n",
    "    # if torch.cuda.is_available():\n",
    "    #     torch.cuda.manual_seed_all(seed)  # 多GPU场景\n",
    "    #     torch.backends.cudnn.deterministic = True  # 确保CUDA卷积操作确定性\n",
    "    #     torch.backends.cudnn.benchmark = False  # 禁用自动优化（可能引入随机性）\n",
    "    \n",
    "    # 如果使用Jittor，固定Jittor的随机种子\n",
    "    jittor.set_seed(seed)\n",
    "\n",
    "\n",
    "# 立即调用，确保在所有随机操作前生效\n",
    "set_all_random_seeds(seed=42)  # 建议使用固定数字（如42），方便复现\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a46fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MY_TOKEN = '<replace with your token>'\n",
    "LOW_RESOURCE = False \n",
    "NUM_DIFFUSION_STEPS = 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "ldm_stable = StableDiffusionPipeline.from_pretrained(\"/root/autodl-tmp/stable-diffusion-v1-4\", use_auth_token=MY_TOKEN)\n",
    "tokenizer = ldm_stable.tokenizer\n",
    "\n",
    "\n",
    "# ## Prompt-to-Prompt Attnetion Controllers\n",
    "# Our main logic is implemented in the `forward` call in an `AttentionControl` object.\n",
    "# The forward is called in each attention layer of the diffusion model and it can modify the input attnetion weights `attn`.\n",
    "# \n",
    "# `is_cross`, `place_in_unet in (\"down\", \"mid\", \"up\")`, `AttentionControl.cur_step` help us track the exact attention layer and timestamp during the diffusion iference.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c15e43",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LocalBlend:\n",
    "\n",
    "    def __call__(self, x_t, attention_store):\n",
    "        k = 1\n",
    "        maps = attention_store[\"down_cross\"][2:4] + attention_store[\"up_cross\"][:3]\n",
    "        maps = [item.reshape(self.alpha_layers.shape[0], -1, 1, 16, 16, MAX_NUM_WORDS) for item in maps]\n",
    "        maps = torch.cat(maps, dim=1)\n",
    "        maps = (maps * self.alpha_layers).sum(-1).mean(1)\n",
    "        mask = nnf.max_pool2d(maps, (k * 2 + 1, k * 2 +1), (1, 1), padding=(k, k))\n",
    "        mask = nnf.interpolate(mask, size=(x_t.shape[2:]))\n",
    "        mask = mask / mask.max(2, keepdims=True)[0].max(3, keepdims=True)[0]\n",
    "        #mask = mask.gt(self.threshold)\n",
    "        mask = mask > self.threshold\n",
    "        \n",
    "        mask = (mask[:1] + mask[1:]).float()\n",
    "        x_t = x_t[:1] + mask * (x_t - x_t[:1])\n",
    "        return x_t\n",
    "       \n",
    "    def __init__(self, prompts: List[str], words: [List[List[str]]], threshold=.3):\n",
    "        alpha_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
    "        for i, (prompt, words_) in enumerate(zip(prompts, words)):\n",
    "            if type(words_) is str:\n",
    "                words_ = [words_]\n",
    "            for word in words_:\n",
    "                ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
    "                alpha_layers[i, :, :, :, :, ind] = 1\n",
    "        self.alpha_layers = alpha_layers\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "class AttentionControl(abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "    \n",
    "    def between_steps(self):\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def num_uncond_att_layers(self):\n",
    "        return self.num_att_layers if LOW_RESOURCE else 0\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        if self.cur_att_layer >= self.num_uncond_att_layers:\n",
    "            if LOW_RESOURCE:\n",
    "                attn = self.forward(attn, is_cross, place_in_unet)\n",
    "            else:\n",
    "                h = attn.shape[0]\n",
    "                attn[h // 2:] = self.forward(attn[h // 2:], is_cross, place_in_unet)\n",
    "        self.cur_att_layer += 1\n",
    "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step += 1\n",
    "            self.between_steps()\n",
    "        return attn\n",
    "    \n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "class EmptyControl(AttentionControl):\n",
    "    \n",
    "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
    "        return attn\n",
    "    \n",
    "    \n",
    "class AttentionStore(AttentionControl):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\"down_cross\": [], \"mid_cross\": [], \"up_cross\": [],\n",
    "                \"down_self\": [],  \"mid_self\": [],  \"up_self\": []}\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
    "        if attn.shape[1] <= 32 ** 2:  # avoid memory overhead\n",
    "            self.step_store[key].append(attn)\n",
    "        return attn\n",
    "\n",
    "    def between_steps(self):\n",
    "        if len(self.attention_store) == 0:\n",
    "            self.attention_store = self.step_store\n",
    "        else:\n",
    "            for key in self.attention_store:\n",
    "                for i in range(len(self.attention_store[key])):\n",
    "                    self.attention_store[key][i] += self.step_store[key][i]\n",
    "        self.step_store = self.get_empty_store()\n",
    "\n",
    "    def get_average_attention(self):\n",
    "        average_attention = {key: [item / self.cur_step for item in self.attention_store[key]] for key in self.attention_store}\n",
    "        return average_attention\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        super(AttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionStore, self).__init__()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "        \n",
    "class AttentionControlEdit(AttentionStore, abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        if self.local_blend is not None:\n",
    "            x_t = self.local_blend(x_t, self.attention_store)\n",
    "        return x_t\n",
    "        \n",
    "    def replace_self_attention(self, attn_base, att_replace):\n",
    "        if att_replace.shape[2] <= 16 ** 2:\n",
    "            return attn_base.unsqueeze(0).expand(att_replace.shape[0], *attn_base.shape)\n",
    "        else:\n",
    "            return att_replace\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        super(AttentionControlEdit, self).forward(attn, is_cross, place_in_unet)\n",
    "        if is_cross or (self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]):\n",
    "            h = attn.shape[0] // (self.batch_size)\n",
    "            attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
    "            attn_base, attn_repalce = attn[0], attn[1:]\n",
    "            if is_cross:\n",
    "                alpha_words = self.cross_replace_alpha[self.cur_step]\n",
    "                attn_repalce_new = self.replace_cross_attention(attn_base, attn_repalce) * alpha_words + (1 - alpha_words) * attn_repalce\n",
    "                \n",
    "                \n",
    "                # print(\"attn[1:].shape（原4维）:\", attn[1:].shape)  # 例如：(2, 4, 64, 64)\n",
    "                # print(\"attn_repalce_new.shape（5维）:\", attn_repalce_new.shape)  # 例如：(2, 3, 4, 64, 64)\n",
    "                if attn_repalce_new.ndim == 5:\n",
    "                    # 如果是5维，删除第0维（最开始的一维）\n",
    "                    attn_repalce_new = attn_repalce_new.squeeze(0)  # 用squeeze删除大小为1的维度\n",
    "                    # 额外检查：删除后是否变为4维，且形状与目标切片匹配\n",
    "                    assert attn_repalce_new.ndim == 4, \"删除第0维后仍不是4维！\"\n",
    "                    assert attn_repalce_new.shape == attn[1:].shape, \"删除维度后形状仍不匹配！\"\n",
    "                \n",
    "                attn[1:] = attn_repalce_new\n",
    "            else:\n",
    "                attn[1:] = self.replace_self_attention(attn_base, attn_repalce)\n",
    "            attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
    "        return attn\n",
    "    \n",
    "    def __init__(self, prompts, num_steps: int,\n",
    "                 cross_replace_steps: Union[float, Tuple[float, float], Dict[str, Tuple[float, float]]],\n",
    "                 self_replace_steps: Union[float, Tuple[float, float]],\n",
    "                 local_blend: Optional[LocalBlend]):\n",
    "        super(AttentionControlEdit, self).__init__()\n",
    "        self.batch_size = len(prompts)\n",
    "        self.cross_replace_alpha = ptp_utils.get_time_words_attention_alpha(prompts, num_steps, cross_replace_steps, tokenizer)\n",
    "        if type(self_replace_steps) is float:\n",
    "            self_replace_steps = 0, self_replace_steps\n",
    "        self.num_self_replace = int(num_steps * self_replace_steps[0]), int(num_steps * self_replace_steps[1])\n",
    "        self.local_blend = local_blend\n",
    "\n",
    "class AttentionReplace(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        return torch.einsum('hpw,bwn->bhpn', attn_base, self.mapper)\n",
    "      \n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
    "                 local_blend: Optional[LocalBlend] = None):\n",
    "        super(AttentionReplace, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.mapper = seq_aligner.get_replacement_mapper(prompts, tokenizer)\n",
    "        \n",
    "\n",
    "class AttentionRefine(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        attn_base_replace = attn_base[:, :, self.mapper].permute(2, 0, 1, 3)\n",
    "        attn_replace = attn_base_replace * self.alphas + att_replace * (1 - self.alphas)\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
    "                 local_blend: Optional[LocalBlend] = None):\n",
    "        super(AttentionRefine, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.mapper, alphas = seq_aligner.get_refinement_mapper(prompts, tokenizer)\n",
    "        self.mapper, alphas = self.mapper, alphas\n",
    "        self.alphas = alphas.reshape(alphas.shape[0], 1, 1, alphas.shape[1])\n",
    "\n",
    "\n",
    "class AttentionReweight(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        if self.prev_controller is not None:\n",
    "            attn_base = self.prev_controller.replace_cross_attention(attn_base, att_replace)\n",
    "        attn_replace = attn_base[None, :, :, :] * self.equalizer[:, None, None, :]\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float, equalizer,\n",
    "                local_blend: Optional[LocalBlend] = None, controller: Optional[AttentionControlEdit] = None):\n",
    "        super(AttentionReweight, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.equalizer = equalizer\n",
    "        self.prev_controller = controller\n",
    "\n",
    "\n",
    "def get_equalizer(text: str, word_select: Union[int, Tuple[int, ...]], values: Union[List[float],\n",
    "                  Tuple[float, ...]]):\n",
    "    if type(word_select) is int or type(word_select) is str:\n",
    "        word_select = (word_select,)\n",
    "    equalizer = torch.ones(len(values), 77)\n",
    "    values = torch.tensor(values, dtype=torch.float32)\n",
    "    for word in word_select:\n",
    "        inds = ptp_utils.get_word_inds(text, word, tokenizer)\n",
    "        equalizer[:, inds] = values\n",
    "    return equalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fedb5e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def aggregate_attention(attention_store: AttentionStore, res: int, from_where: List[str], is_cross: bool, select: int):\n",
    "    out = []\n",
    "    attention_maps = attention_store.get_average_attention()\n",
    "    num_pixels = res ** 2\n",
    "    for location in from_where:\n",
    "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
    "            if item.shape[1] == num_pixels:\n",
    "                cross_maps = item.reshape(len(prompts), -1, res, res, item.shape[-1])[select]\n",
    "                out.append(cross_maps)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    out = out.sum(0) / out.shape[0]\n",
    "    return out.cpu()\n",
    "\n",
    "\n",
    "def show_cross_attention(attention_store: AttentionStore, res: int, from_where: List[str], select: int = 0):\n",
    "    tokens = tokenizer.encode(prompts[select])\n",
    "    decoder = tokenizer.decode\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, True, select)\n",
    "    images = []\n",
    "    for i in range(len(tokens)):\n",
    "        image = attention_maps[:, :, i]\n",
    "        image = 255 * image / image.max()\n",
    "        image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
    "        image = image.numpy().astype(np.uint8)\n",
    "        image = np.array(Image.fromarray(image).resize((256, 256)))\n",
    "        image = ptp_utils.text_under_image(image, decoder(int(tokens[i])))\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.stack(images, axis=0))\n",
    "    \n",
    "\n",
    "def show_self_attention_comp(attention_store: AttentionStore, res: int, from_where: List[str],\n",
    "                        max_com=10, select: int = 0):\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, False, select).numpy().reshape((res ** 2, res ** 2))\n",
    "    u, s, vh = np.linalg.svd(attention_maps - np.mean(attention_maps, axis=1, keepdims=True))\n",
    "    images = []\n",
    "    for i in range(max_com):\n",
    "        image = vh[i].reshape(res, res)\n",
    "        image = image - image.min()\n",
    "        image = 255 * image / image.max()\n",
    "        image = np.repeat(np.expand_dims(image, axis=2), 3, axis=2).astype(np.uint8)\n",
    "        image = Image.fromarray(image).resize((256, 256))\n",
    "        image = np.array(image)\n",
    "        images.append(image)\n",
    "    ptp_utils.view_images(np.concatenate(images, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06257a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_and_display(prompts, controller, latent=None, run_baseline=False, generator=None):\n",
    "    if run_baseline:\n",
    "        print(\"w.o. prompt-to-prompt\")\n",
    "        images, latent = run_and_display(prompts, EmptyControl(), latent=latent, run_baseline=False, generator=generator)\n",
    "        print(\"with prompt-to-prompt\")\n",
    "    images, x_t = ptp_utils.text2image_ldm_stable(ldm_stable, prompts, controller, latent=latent, num_inference_steps=NUM_DIFFUSION_STEPS, guidance_scale=GUIDANCE_SCALE, generator=generator, low_resource=LOW_RESOURCE)\n",
    "    ptp_utils.view_images(images)\n",
    "    return images, x_t\n",
    "\n",
    "\n",
    "# ## Cross-Attention Visualization\n",
    "# First let's generate an image and visualize the cross-attention maps for each word in the prompt.\n",
    "# Notice, we normalize each map to 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "g_cpu = torch.Generator().manual_seed(8888)\n",
    "prompts = [\"A painting of a squirrel eating a burger\"]\n",
    "controller = AttentionStore()\n",
    "image, x_t = run_and_display(prompts, controller, latent=None, run_baseline=False, generator=g_cpu)\n",
    "show_cross_attention(controller, res=16, from_where=(\"up\", \"down\"))\n",
    "\n",
    "\n",
    "# ## Replacement edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f6c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = [\"A painting of a squirrel eating a burger\",\n",
    "           \"A painting of a lion eating a burger\"]\n",
    "\n",
    "controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8, self_replace_steps=0.4)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=True)\n",
    "\n",
    "\n",
    "# ### Modify Cross-Attention injection #steps for specific words\n",
    "# Next, we can reduce the restriction on our lion by reducing the number of cross-attention injection with respect to the word \"lion\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad0c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = [\"A painting of a squirrel eating a burger\",\n",
    "           \"A painting of a lion eating a burger\"]\n",
    "controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps={\"default_\": 1., \"lion\": .4},\n",
    "                              self_replace_steps=0.4)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)\n",
    "\n",
    "\n",
    "# ### Local Edit\n",
    "# Lastly, if we want to preseve the original burger, we can apply a local edit with respect to the squirrel and the lion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67bc7ec",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = [\"A painting of a squirrel eating a burger\",\n",
    "           \"A painting of a lion eating a burger\"]\n",
    "lb = LocalBlend(prompts, (\"squirrel\", \"lion\"))\n",
    "controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS,\n",
    "                              cross_replace_steps={\"default_\": 1., \"lion\": .4},\n",
    "                              self_replace_steps=0.4, local_blend=lb)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b8a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = [\"A painting of a squirrel eating a burger\",\n",
    "           \"A painting of a squirrel eating a lasagne\"]\n",
    "lb = LocalBlend(prompts, (\"burger\", \"lasagne\"))\n",
    "controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS,\n",
    "                              cross_replace_steps={\"default_\": 1., \"lasagne\": .2},\n",
    "                              self_replace_steps=0.4,\n",
    "                              local_blend=lb)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=True)\n",
    "\n",
    "\n",
    "# ## Refinement edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b39cd4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = [\"A painting of a squirrel eating a burger\",\n",
    "           \"A neoclassical painting of a squirrel eating a burger\"]\n",
    "\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS,\n",
    "                             cross_replace_steps=.5, \n",
    "                             self_replace_steps=.2)\n",
    "_ = run_and_display(prompts, controller, latent=x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb3379b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = [\"a photo of a house on a mountain\",\n",
    "           \"a photo of a house on a mountain at fall\"]\n",
    "\n",
    "\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                             self_replace_steps=.4)\n",
    "_ = run_and_display(prompts, controller, latent=x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28540396",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = [\"a photo of a house on a mountain\",\n",
    "           \"a photo of a house on a mountain at winter\"]\n",
    "\n",
    "\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                             self_replace_steps=.4)\n",
    "_ = run_and_display(prompts, controller, latent=x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c12321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = [\"soup\",\n",
    "           \"pea soup\"] \n",
    "\n",
    "lb = LocalBlend(prompts, (\"soup\", \"soup\"))\n",
    "\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                             self_replace_steps=.4,\n",
    "                             local_blend=lb)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)\n",
    "\n",
    "\n",
    "# ## Attention Re-Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3fca51",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = [\"a smiling bunny doll\"] * 2\n",
    "\n",
    "### pay 3 times more attention to the word \"smiling\"\n",
    "equalizer = get_equalizer(prompts[1], (\"smiling\",), (5,))\n",
    "controller = AttentionReweight(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                               self_replace_steps=.4,\n",
    "                               equalizer=equalizer)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = [\"pink bear riding a bicycle\"] * 2\n",
    "\n",
    "### we don't wont pink bikes, only pink bear.\n",
    "### we reduce the amount of pink but apply it locally on the bikes (attention re-weight + local mask )\n",
    "\n",
    "### pay less attention to the word \"pink\"\n",
    "equalizer = get_equalizer(prompts[1], (\"pink\",), (-1,))\n",
    "\n",
    "### apply the edit on the bikes \n",
    "lb = LocalBlend(prompts, (\"bicycle\", \"bicycle\"))\n",
    "controller = AttentionReweight(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                               self_replace_steps=.4,\n",
    "                               equalizer=equalizer,\n",
    "                               local_blend=lb)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)\n",
    "\n",
    "\n",
    "# ### Where are my croutons?\n",
    "# It might be useful to use Attention Re-Weighting with a previous edit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075faeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = [\"soup\",\n",
    "           \"pea soup with croutons\"] \n",
    "lb = LocalBlend(prompts, (\"soup\", \"soup\"))\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                             self_replace_steps=.4, local_blend=lb)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)\n",
    "\n",
    "\n",
    "# Now, with more attetnion to `\"croutons\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48344013",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = [\"soup\",\n",
    "           \"pea soup with croutons\"] \n",
    "\n",
    "\n",
    "lb = LocalBlend(prompts, (\"soup\", \"soup\"))\n",
    "controller_a = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8, \n",
    "                               self_replace_steps=.4, local_blend=lb)\n",
    "\n",
    "### pay 3 times more attention to the word \"croutons\"\n",
    "equalizer = get_equalizer(prompts[1], (\"croutons\",), (3,))\n",
    "controller = AttentionReweight(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                               self_replace_steps=.4, equalizer=equalizer, local_blend=lb,\n",
    "                               controller=controller_a)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6febdcc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = [\"potatos\",\n",
    "           \"fried potatos\"] \n",
    "lb = LocalBlend(prompts, (\"potatos\", \"potatos\"))\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                             self_replace_steps=.4, local_blend=lb)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4fcdde",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = [\"potatos\",\n",
    "           \"fried potatos\"] \n",
    "lb = LocalBlend(prompts, (\"potatos\", \"potatos\"))\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8, \n",
    "                             self_replace_steps=.4, local_blend=lb)\n",
    "\n",
    "### pay 10 times more attention to the word \"fried\"\n",
    "equalizer = get_equalizer(prompts[1], (\"fried\",), (10,))\n",
    "controller = AttentionReweight(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                               self_replace_steps=.4, equalizer=equalizer, local_blend=lb,\n",
    "                               controller=controller_a)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f354b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (cross-attention)",
   "language": "python",
   "name": "cross-attention"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
